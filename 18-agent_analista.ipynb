{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(\"https://datahub.io/core/global-temp/r/0.csv\")\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "agent_temp = create_pandas_dataframe_agent(\n",
    "    chat,\n",
    "    df_temp,\n",
    "    verbose=True,\n",
    "    agent_type=\"tool-calling\",\n",
    "    allow_dangerous_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Qual temperatura média global do último ano\n",
    "resposta = agent_temp.invoke({\"input\":\"Qual a temperatura média global do último ano?\"})\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- QUal foi o ano com a maior temperatura média?\n",
    "resposta = agent_temp.invoke({\"input\":\"Qual foi o ano com maior temperatura média?\"})\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.base import create_sql_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_sakila = SQLDatabase.from_uri(\"sqlite:///files/sakila_master.db\")\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "agent_executor_sakila = create_sql_agent(\n",
    "    chat,\n",
    "    db=db_sakila,\n",
    "    agent_type=\"tool-calling\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor_sakila.invoke({\"input\": \"Quais são os filmes mais alugados?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor_sakila.invoke({\"input\": \"Quais atores atuaram em mais filmes?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor_sakila.invoke({\"input\": \"Qual é o cliente com mais aluguéis?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
